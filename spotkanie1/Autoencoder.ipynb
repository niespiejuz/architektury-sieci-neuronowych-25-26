{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGu5Rr_yOnQL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalizacja wartości pikseli do [0,1]\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Dodawanie wymiaru (wymagane przez framework do treningu CNN)\n",
        "x_train = np.expand_dims(x_train, -1)  # (60000, 28, 28) -> (60000, 28, 28, 1)\n",
        "x_test = np.expand_dims(x_test, -1)  # (10000, 28, 28) -> (10000, 28, 28, 1)\n",
        "\n",
        "print(x_train.shape, x_test.shape)"
      ],
      "metadata": {
        "id": "FpMp1bbjOw_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dodawanie losowego szumu do obrazów\n",
        "noise_factor = 0.5\n",
        "\n",
        "x_train_noisy = x_train + noise_factor * np.random.normal(size=x_train.shape)\n",
        "x_test_noisy = x_test + noise_factor * np.random.normal(size=x_test.shape)\n",
        "\n",
        "# Przycinanie wartości do zakresu [0, 1]\n",
        "x_train_noisy = np.clip(x_train_noisy, 0.0, 1.0)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0.0, 1.0)"
      ],
      "metadata": {
        "id": "JbS4FyaFOz1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Budowa prostego modelu autoencoder\n",
        "autoencoder = keras.Sequential([\n",
        "    layers.Input(shape=(28, 28, 1)),\n",
        "\n",
        "    layers.Conv2D(16, 3, activation=\"relu\", padding=\"same\"),\n",
        "    layers.MaxPooling2D(2, padding=\"same\"),\n",
        "    layers.Conv2D(8, 3, activation=\"relu\", padding=\"same\"),\n",
        "    layers.MaxPooling2D(2, padding=\"same\"),\n",
        "\n",
        "    layers.Conv2D(8, 3, activation=\"relu\", padding=\"same\"),\n",
        "    layers.UpSampling2D(2),\n",
        "    layers.Conv2D(16, 3, activation=\"relu\", padding=\"same\"),\n",
        "    layers.UpSampling2D(2),\n",
        "    layers.Conv2D(1, 3, activation=\"sigmoid\", padding=\"same\")\n",
        "])\n",
        "\n",
        "autoencoder.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"binary_crossentropy\"\n",
        ")\n",
        "\n",
        "autoencoder.summary()"
      ],
      "metadata": {
        "id": "3BBAdV3FO3-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit(\n",
        "    x_train_noisy,\n",
        "    x_train,\n",
        "    epochs=5,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    validation_data=(x_test_noisy[10:], x_test[10:]) # jako, że nie mamy osobnego validation set, użyję prawie wszystkich obrazów z test set,\n",
        "                                                     # pozostawiając pierwsze 10 na faktyczną predykcję (nie użyję ich do walidacji) - na potrzeby przykładu\n",
        "                                                     # te 10 obrazów wystarczy, by obejrzeć ostateczne działanie autoencodera\n",
        ")"
      ],
      "metadata": {
        "id": "n6HFmAruO-V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_images = autoencoder.predict(x_test_noisy[:10])\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "for i in range(10):\n",
        "    ax = plt.subplot(3, 10, i + 1)\n",
        "    plt.imshow(x_test_noisy[i].squeeze(), cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    if i == 0:\n",
        "        ax.set_ylabel(\"Noisy\")\n",
        "\n",
        "    ax = plt.subplot(3, 10, i + 11)\n",
        "    plt.imshow(decoded_images[i].squeeze(), cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    if i == 0:\n",
        "        ax.set_ylabel(\"Denoised\")\n",
        "\n",
        "    ax = plt.subplot(3, 10, i + 21)\n",
        "    plt.imshow(x_test[i].squeeze(), cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    if i == 0:\n",
        "        ax.set_ylabel(\"Original\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ubYTPf2jPykQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}