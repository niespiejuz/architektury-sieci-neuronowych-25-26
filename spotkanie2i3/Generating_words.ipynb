{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfF1204n4ZNV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "i love ai\n",
        "i love deep learning\n",
        "i love machine learning\n",
        "i love neural networks\n",
        "machine learning is fun\n",
        "ai is fun\n",
        "learning ai is important\n",
        "i build ai projects\n",
        "ai can learn patterns\n",
        "deep ai is powerful\n",
        "ai will change the world\n",
        "practice ai every day\n",
        "models help ai learn\n",
        "ai needs good data\n",
        "understanding ai is key\n",
        "training ai takes time\n",
        "optimizers improve ai\n",
        "ai can solve problems\n",
        "ai improves with practice\n",
        "evaluation of ai matters\n",
        "data makes ai better\n",
        "ai helps humans\n",
        "building ai is enjoyable\n",
        "i enjoy learning ai\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WWV7cRQQ4ewA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "print(\"Word index: \", tokenizer.word_index)"
      ],
      "metadata": {
        "id": "QuJfRx0t8LiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = []\n",
        "\n",
        "for line in text.strip().split(\"\\n\"):\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1, len(token_list)):\n",
        "    sequences.append(token_list[:i+1])\n",
        "\n",
        "max_len = max([len(x) for x in sequences])\n",
        "sequences = pad_sequences(sequences, maxlen=max_len, padding=\"pre\")\n",
        "print(\"Sequences: \", sequences)"
      ],
      "metadata": {
        "id": "wQnOW4OV-B6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = sequences[:, :-1]\n",
        "y = sequences[:, -1]\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=len(tokenizer.word_index)+1)"
      ],
      "metadata": {
        "id": "5Cg2jmppCHRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(len(tokenizer.word_index)+1, 20),\n",
        "    tf.keras.layers.GRU(64),\n",
        "    tf.keras.layers.Dense(len(tokenizer.word_index)+1, activation=\"softmax\"),\n",
        "])"
      ],
      "metadata": {
        "id": "2iJHVjGaFHit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "YMhryMeLHFED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=100, verbose=0)"
      ],
      "metadata": {
        "id": "myVoNRIIHNGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_words(seed_text, next_words):\n",
        "  for _ in range(next_words):\n",
        "    seed_token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    seed_token_list = pad_sequences([seed_token_list], maxlen=max_len-1, padding=\"pre\")\n",
        "\n",
        "    next_word_index = np.argmax(model.predict(seed_token_list, verbose=0), axis=-1)\n",
        "    next_word = tokenizer.index_word[next_word_index[0]]\n",
        "    seed_text += \" \" + next_word\n",
        "  return seed_text"
      ],
      "metadata": {
        "id": "gZx1aQgfH33_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_words_top3(seed_text, next_words):\n",
        "  for _ in range(next_words):\n",
        "    seed_token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    seed_token_list = pad_sequences([seed_token_list], maxlen=max_len-1, padding=\"pre\")\n",
        "\n",
        "    predicts = model.predict(seed_token_list, verbose=0)[0]\n",
        "    top_words_indices = np.argsort(predicts)[-3:][::-1]\n",
        "\n",
        "    next_word_index = np.random.choice(top_words_indices)\n",
        "    next_word = tokenizer.index_word[next_word_index]\n",
        "\n",
        "    seed_text += \" \" + next_word\n",
        "  return seed_text"
      ],
      "metadata": {
        "id": "-VWJHz-4K-0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_words_top3(\"i love\", 6))"
      ],
      "metadata": {
        "id": "q3ks5YYlKSqa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}